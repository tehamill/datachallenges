{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import roc_auc_score, r2_score, mean_absolute_error, mean_squared_error\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"recipies.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Domain Specific Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stops = [ 'vegetable_oil','all_purpose_flour', 'butter', 'green_onion', 'purple_onion', 'salt', 'chili_powder',\n",
    " 'red_bell_pepper', 'extra_virgin_olive_oil', 'ginger', 'black_pepper', 'milk', 'oil', 'all',\n",
    " 'egg', 'scallion', 'grated_parmesan_cheese', 'corn_starch', 'olive_oil', 'water', 'unsalted_butter',\n",
    " 'soy_sauce', 'baking_powder', 'garlic', 'pepper', 'kosher_salt', 'carrot', 'cinnamon', 'extra', 'sugar', 'onion', 'garlic_clove', 'tomatoe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dashes(listofstrings):\n",
    "    return [x.replace(\"-\", \" \") for x in listofstrings]\n",
    "\n",
    "def describe_cat(X):\n",
    "    \"\"\"function to show categorical variables of a dataframe\"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(X[X.columns[X.dtypes==\"object\"]].describe().to_html()))\n",
    "def connect_strings(listofstrings):\n",
    "    return [x.replace(\" \", \"_\") for x in listofstrings]\n",
    "\n",
    "def describe_cat(X):\n",
    "    \"\"\"function to show categorical variables of a dataframe\"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(X[X.columns[X.dtypes==\"object\"]].describe().to_html()))\n",
    "\n",
    "def connect_list(listofstrings):\n",
    "    return \" \".join(listofstrings)\n",
    "\n",
    "def clean_words(listofstrings):\n",
    "    listofstrings = [x[:-len('s')] if x.endswith('s') else x for x in listofstrings]\n",
    "    words = [x[len('ground '):] if x.startswith('ground') else x for x in listofstrings]\n",
    "    words = ['egg' if 'egg' in x else x for x in words]\n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Data:\n",
    "    1. Remove Dashes\n",
    "    2. Equate similar words (ground cinnamon = cinnamon)\n",
    "    3. Connect single ingredients with \"_\"\n",
    "    4. Connect list of ingredients into single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['ingredients'] = df['ingredients'].apply(remove_dashes)\n",
    "\n",
    "df['ingredients'] = df['ingredients'].apply(clean_words)\n",
    "\n",
    "df['ingredients'] = df['ingredients'].apply(connect_strings)\n",
    "\n",
    "df['ingredients'] = df['ingredients'].apply(connect_list)\n",
    "\n",
    "\n",
    "df_y = df['cuisine']\n",
    "df_x= df['ingredients']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get class with smallest number of entries and ballance the data set (each class has same number of entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['cuisine'].value_counts()\n",
    "cuisine_types = counts.index\n",
    "\n",
    "size = counts.min()-2       # sample size\n",
    "replace = False  # with replacement\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "ballanced = df.groupby('cuisine', as_index=False).apply(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfY=ballanced['cuisine']\n",
    "dfX=ballanced['ingredients']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dfX,dfY,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the words using tf idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv2 = TfidfVectorizer(min_df=1,stop_words = my_stops)\n",
    "x_train_tfv2 = tfv2.fit_transform(x_train.values)\n",
    "x_test_tfv2 = tfv2.transform(x_test.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encode target variable classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(cuisine_types)\n",
    "y_train_en =  le.transform(y_train) \n",
    "y_test_en =  le.transform(y_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Random Forest and get accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=16, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=4, min_samples_split=4,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=2000, max_depth=16,min_samples_split=4,min_samples_leaf=4,\n",
    "                              random_state=0)\n",
    "clf.fit(x_train_tfv2, y_train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pred_train = clf.predict(x_train_tfv2)\n",
    "RF_pred_test = clf.predict(x_test_tfv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5448924731182796"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train_en,RF_pred_train)\n",
    "#TRAINING ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5016129032258064"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_en,RF_pred_test)\n",
    "#TESTING ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Gaussian Naive Bayes and get accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB(var_smoothing=1.)\n",
    "gbmod= gnb.fit(x_train_tfv2.toarray(), y_train_en)\n",
    "nbpred = gbmod.predict(x_train_tfv2.toarray())\n",
    "nbpredT = gbmod.predict(x_test_tfv2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8133064516129033"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_pred_train = gbmod.predict(x_train_tfv2.toarray())\n",
    "accuracy_score(y_train_en,NB_pred_train)\n",
    "#TRAINING ACCURACY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6435483870967742"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_pred_test = gbmod.predict(x_test_tfv2.toarray())\n",
    "accuracy_score(y_test_en,NB_pred_test)\n",
    "#TESTING ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List out most important words for each cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brazilian  :  ['lime_juice', 'chocolate_sprinkle', 'ice', 'ice_cube', 'tapioca_flour', 'black_bean', 'sweetened_condensed_milk', 'lime', 'cachaca', 'coconut_milk']\n",
      "british  :  ['nutmeg', 'self_rising_flour', 'potatoe', 'worcestershire_sauce', 'baking_soda', 'heavy_cream', 'vanilla_extract', 'flour', 'white_sugar', 'whole_milk']\n",
      "cajun_creole  :  ['diced_tomatoe', 'hot_sauce', 'bay_leave', 'celery_rib', 'andouille_sausage', 'cayenne_pepper', 'creole_seasoning', 'cajun_seasoning', 'celery', 'green_bell_pepper']\n",
      "chinese  :  ['light_soy_sauce', 'peanut_oil', 'peeled_fresh_ginger', 'fresh_ginger', 'hoisin_sauce', 'oyster_sauce', 'rice_vinegar', 'dark_soy_sauce', 'white_pepper', 'sesame_oil']\n",
      "filipino  :  ['evaporated_milk', 'pork_belly', 'pork', 'vinegar', 'cooking_oil', 'coconut_milk', 'peppercorn', 'fish_sauce', 'bay_leave', 'brown_sugar']\n",
      "french  :  ['leek', 'dijon_mustard', 'frozen_pastry_puff_sheet', 'fresh_lemon_juice', 'shallot', 'whipping_cream', 'heavy_cream', 'whole_milk', 'vanilla_extract', 'dry_white_wine']\n",
      "greek  :  ['fresh_oregano', 'fresh_lemon_juice', 'feta_cheese', 'lemon_juice', 'lemon', 'cucumber', 'fresh_dill', 'greek_yogurt', 'dried_oregano', 'feta_cheese_crumble']\n",
      "indian  :  ['plain_yogurt', 'ghee', 'tumeric', 'green_chilie', 'curry_powder', 'coriander', 'cumin', 'garam_masala', 'turmeric', 'cumin_seed']\n",
      "irish  :  ['russet_potatoe', 'cooking_spray', 'flour', 'mashed_potatoe', 'raisin', 'potatoe', 'irish_whiskey', 'buttermilk', 'cabbage', 'baking_soda']\n",
      "italian  :  ['plum_tomatoe', 'fresh_basil_leave', 'parmesan_cheese', 'flat_leaf_parsley', 'dried_oregano', 'shredded_mozzarella_cheese', 'italian_seasoning', 'dry_white_wine', 'fresh_basil', 'fresh_parsley']\n",
      "jamaican  :  ['scotch_bonnet_chile', 'flour', 'fresh_thyme', 'coconut_milk', 'brown_sugar', 'curry_powder', 'allspice', 'nutmeg', 'thyme', 'dried_thyme']\n",
      "japanese  :  ['miso_paste', 'white_miso', 'sesame_oil', 'mirin', 'dashi', 'nori', 'rice_vinegar', 'sesame_seed', 'fresh_ginger', 'sake']\n",
      "korean  :  ['beef', 'fresh_ginger', 'minced_garlic', 'brown_sugar', 'rice_vinegar', 'kimchi', 'toasted_sesame_seed', 'gochujang_base', 'sesame_oil', 'sesame_seed']\n",
      "mexican  :  ['lime', 'shredded_cheddar_cheese', 'chopped_cilantro_fresh', 'flour_tortilla', 'jalapeno_chilie', 'sour_cream', 'salsa', 'corn_tortilla', 'cumin', 'avocado']\n",
      "moroccan  :  ['chopped_cilantro_fresh', 'cayenne_pepper', 'lemon', 'fresh_lemon_juice', 'paprika', 'coriander', 'tumeric', 'cumin', 'couscou', 'chickpea']\n",
      "russian  :  ['lemon', 'cabbage', 'raisin', 'active_dry_yeast', 'dill', 'fresh_dill', 'beet', 'potatoe', 'flour', 'sour_cream']\n",
      "southern_us  :  ['flour', 'cornmeal', 'bourbon_whiskey', 'granulated_sugar', 'heavy_cream', 'buttermilk', 'baking_soda', 'yellow_corn_meal', 'peache', 'vanilla_extract']\n",
      "spanish  :  ['large_garlic_clove', 'manchego_cheese', 'lemon', 'dry_white_wine', 'orange', 'sherry_vinegar', 'flat_leaf_parsley', 'saffron_thread', 'paprika', 'fresh_parsley']\n",
      "thai  :  ['red_curry_paste', 'lemongras', 'lime', 'lime_juice', 'brown_sugar', 'fish_sauce', 'shallot', 'peanut', 'coconut_milk', 'fresh_lime_juice']\n",
      "vietnamese  :  ['cucumber', 'cilantro_leave', 'rice_vinegar', 'canola_oil', 'lime_juice', 'fresh_lime_juice', 'fish_sauce', 'beansprout', 'lemongras', 'shallot']\n"
     ]
    }
   ],
   "source": [
    "words2 = tfv2.get_feature_names()\n",
    "cuslist = le.inverse_transform(range(len(cuisine_types)))\n",
    "for ine in range(len(cuisine_types)):\n",
    "    cuis_max = np.argpartition(gbmod.theta_[ine], -10)[-10:]\n",
    "    most_imp = [words2[ii] for ii in cuis_max]\n",
    "    print(cuslist[ine], \" : \", most_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = NB_optimal.predict_proba(X_test)\n",
    "words = np.take(count_vect.get_feature_names(), pred_proba.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "optimal_alpha=1\n",
    "NB_optimal = BernoulliNB(alpha=optimal_alpha)\n",
    "\n",
    "# fitting the model\n",
    "NB_optimal.fit(x_train_tfv2.toarray(),y_train_en)\n",
    "\n",
    "# predict the response\n",
    "pred = NB_optimal.predict(x_train_tfv2.toarray())\n",
    "\n",
    "# evaluate accuracy\n",
    "acc = accuracy_score(y_train_en,pred)\n",
    "print('\\nThe accuracy of the NB classifier for k = %d is %f%%' % (optimal_alpha, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtest = NB_optimal.predict(x_test_tfv2.toarray())\n",
    "\n",
    "# evaluate accuracy\n",
    "acc = accuracy_score(y_test_en,predtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ingredients'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.score(x_train_tfv2.toarray(),y_train_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.score(x_test_tfv2.toarray(),y_test_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
